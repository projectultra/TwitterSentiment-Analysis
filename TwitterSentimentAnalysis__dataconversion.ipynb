{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "1pxTCW6fOmg-"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "#package import\n",
        "train=pd.read_csv('/content/Data/twitter_training.csv',names=[\"Tweet_ID\", \"Entity\",\"Sentiment\", \"Tweet_Content\"])\n",
        "test=pd.read_csv('/content/Data/twitter_validation.csv',names=[\"Tweet_ID\", \"Entity\", \"Sentiment\", \"Tweet_Content\"])\n",
        "#data import\n",
        "#refining the data\n",
        "train=train.dropna()\n",
        "#function to clean data\n",
        "def cleannuptext(txt):\n",
        "  #delete 'pattern'\n",
        "  alltweets=txt.append(None,ignore_index = True)\n",
        "  alltweets['tidy_tweet']=np.vectorize(remove_unwanted)(alltweets['Tweet_Content'],'@[\\w]*')\n",
        "  #remove usernames\n",
        "  alltweets['tidy_tweet'] = alltweets['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "  #remove punctuation etc\n",
        "  alltweets['tidy_tweet'] = alltweets['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split()if len(w) > 3]))\n",
        "  #remove short words\n",
        "  #alltweets.head()\n",
        "  tokenized_tweet = alltweets['tidy_tweet'].apply(lambda x: x.split())\n",
        "  tokenized_tweet.head()\n",
        "  #tokenize the words\n",
        "  lemma = nltk.stem.porter.PorterStemmer()\n",
        "  tokenized_tweet = tokenized_tweet.apply(lambda x:[lemma.stem(i) for i in x])\n",
        "  tokenized_tweet.head()\n",
        "  #lemmatizing\n",
        "  for i in range(len(tokenized_tweet)):\n",
        "      tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n",
        "  alltweets['tidy_tweet'] = tokenized_tweet\n",
        "  alltweets.head()\n",
        "  return alltweets\n",
        "def remove_unwanted(txt,pattern):\n",
        "    matches = re.findall(pattern,txt)\n",
        "    for match in matches:\n",
        "      txt = re.sub(match,'',txt)\n",
        "    return txt\n",
        "def toknizetwt(txt):\n",
        "  labels = np.array(txt['Sentiment'])\n",
        "  y = []\n",
        "  for i in range(len(labels)):\n",
        "    if labels[i] == 'Neutral':\n",
        "      y.append(0)\n",
        "    if labels[i] == 'Negative':\n",
        "      y.append(1)\n",
        "    if labels[i] == 'Positive':\n",
        "      y.append(2)\n",
        "    if labels[i] == 'Irrelevant':\n",
        "      y.append(-1)\n",
        "  y = np.array(y)\n",
        "  return y\n",
        "train_data=cleannuptext(train)\n",
        "test_data=cleannuptext(test)\n",
        "train_data.drop(labels=['Tweet_Content','Tweet_ID','Entity'],axis=1,inplace=True)\n",
        "test_data.drop(labels=['Tweet_ID','Entity','Tweet_Content'],axis=1,inplace=True)\n",
        "train_data['Sentiment']=toknizetwt(train_data)\n",
        "test_data['Sentiment']=toknizetwt(test_data)\n",
        "#final test and train dataset\n",
        "train_data.to_csv('training_dataset.csv', index=False)\n",
        "test_data.to_csv('testing_dataset.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "c795d63dc8cfb5de7bcd07d29b5ba58277844ae54d77b796ffde21eff2f01dd2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
